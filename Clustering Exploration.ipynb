{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring representative tuples by clustering the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "from sklearn.cluster import KMeans, Birch\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform clustering and return the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModelPath = 'amazonModelWord2Vec.w2v'\n",
    "fastTextModelPath = 'amazonModelFastText.w2v'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterCentersWithKMeans(model, numberOfClusters):\n",
    "    # Get the word vectors of the model\n",
    "    word_vectors = model.wv.syn0\n",
    "    n_words = word_vectors.shape[0]\n",
    "    vec_size = word_vectors.shape[1]\n",
    "    print(\"Number of words = {0}, vector size = {1}\".format(n_words, vec_size))\n",
    "\n",
    "    # Cluster using KMeans\n",
    "    start = time.time()\n",
    "    print(\"Clustering ... \", end=\"\", flush=True)\n",
    "    kmeans = KMeans(n_clusters=numberOfClusters, n_jobs=-1, random_state=0)\n",
    "    idx = kmeans.fit_predict(word_vectors)\n",
    "    print(\"Finished clustering in {:.2f} sec.\".format(time.time() - start), flush=True)\n",
    "\n",
    "    # Return cluster centers\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birch Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClusterCentersWithBirch(model, numberOfClusters):\n",
    "    # Get the word vectors of the model\n",
    "    word_vectors = model.wv.syn0\n",
    "    n_words = word_vectors.shape[0]\n",
    "    vec_size = word_vectors.shape[1]\n",
    "    print(\"Number of words = {0}, vector size = {1}\".format(n_words, vec_size))\n",
    "\n",
    "    # Cluster using KMeans\n",
    "    start = time.time()\n",
    "    print(\"Clustering ... \", end=\"\", flush=True)\n",
    "    birch = Birch(n_clusters=numberOfClusters)\n",
    "    idx = birch.fit_predict(word_vectors)\n",
    "    print(\"Finished clustering in {:.2f} sec.\".format(time.time() - start), flush=True)\n",
    "\n",
    "    # Return cluster centers\n",
    "    return birch.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the closest vector to each of the cluster centers\n",
    "We'll pass the number of cluster centers as an argument. This can be thought of as a drill down equivalent.\n",
    "\n",
    "Greater the number of cluster centers, more detailed will be the resulting results returned. \n",
    "\n",
    "Number of clusters chosen is 3 by default. This can be overriden, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClosestWordEmbedding(modelPath, numberOfClusters = 3):\n",
    "    # Load the model\n",
    "    start = time.time()\n",
    "    model = KeyedVectors.load(modelPath)\n",
    "    print(\"Finished loading model in {:.2f} sec.\".format(time.time() - start), flush=True)\n",
    "    \n",
    "    clusterCenters = getClusterCentersWithKMeans(model, numberOfClusters)\n",
    "    \n",
    "    # Create an empty numpy array of size equal to cluster centers to store the closest words\n",
    "    closestWords = []\n",
    "    \n",
    "    # Get the closest word for each of the cluster centers\n",
    "    for clusterCenter in clusterCenters:\n",
    "            closestWords.append(model.similar_by_vector(clusterCenter))\n",
    "    \n",
    "    return closestWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model in 0.14 sec.\n",
      "Number of words = 18473, vector size = 100\n",
      "Clustering ... Finished clustering in 3.60 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Pokémon Adventures: Diamond and Pearl/Platinum, Vol. 9',\n",
       "   0.7363650798797607),\n",
       "  ('July 09, 2000', 0.7359441518783569),\n",
       "  ('Gorilla Adventure', 0.7359086275100708),\n",
       "  ('June 04, 2001', 0.7354663610458374),\n",
       "  ('Lion Adventure', 0.7346001863479614),\n",
       "  ('Milda Harris', 0.7345806956291199),\n",
       "  ('The Adventures of Danny Meadow Mouse', 0.7342305183410645),\n",
       "  ('June 18, 2013', 0.7342035174369812),\n",
       "  ('The Adventures of Buster Bear', 0.7334026098251343),\n",
       "  (\"Arthur's Computer Disaster: An Arthur Adventure\", 0.7328624129295349)],\n",
       " [('April 30, 2008', 0.7729446887969971),\n",
       "  (\"Piratica: Being a Daring Tale of a Singular Girl's Adventure Upon the High Seas\",\n",
       "   0.7682402729988098),\n",
       "  ('September 27, 2004', 0.7669798135757446),\n",
       "  ('August 21, 2017', 0.7668962478637695),\n",
       "  ('March 15, 2012', 0.7657756209373474),\n",
       "  ('October 08, 2017', 0.7656553387641907),\n",
       "  ('July 01, 1985', 0.765381932258606),\n",
       "  ('Sybex Inc', 0.7651218175888062),\n",
       "  ('November 09, 2010', 0.7644745707511902),\n",
       "  ('April 19, 2003', 0.7642829418182373)],\n",
       " [('December 13, 2010', 0.7788293361663818),\n",
       "  ('JACE Publishing LLC', 0.7656327486038208),\n",
       "  ('January 04, 2018', 0.764803409576416),\n",
       "  ('Jane Porter', 0.7638676762580872),\n",
       "  ('Inadvertent Adventures', 0.7627156376838684),\n",
       "  ('January 30, 2014', 0.7625996470451355),\n",
       "  ('Girl Online en tournée: Girl Online, Tome 2', 0.7620481252670288),\n",
       "  ('Julie', 0.7612882852554321),\n",
       "  ('March 31, 2012', 0.7610347270965576),\n",
       "  ('Fantasy & Science Fiction, January/February 2015', 0.7609479427337646)]]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getClosestWordEmbedding(word2VecModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model in 0.36 sec.\n",
      "Number of words = 18405, vector size = 100\n",
      "Clustering ... Finished clustering in 3.61 sec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('Ben Caldwell', 0.798562228679657),\n",
       "  ('Carrie Hope Fletcher', 0.7798928022384644),\n",
       "  ('Darryl Bailey', 0.779863715171814),\n",
       "  ('Karen Chance', 0.7791794538497925),\n",
       "  ('Apsley Cherry-Garrard', 0.7774773836135864),\n",
       "  ('Dan Parry', 0.777237057685852),\n",
       "  ('Forever Road (Peri Jean Mace Ghost Thriller)', 0.7764514088630676),\n",
       "  ('Sophie Chen Keller', 0.7706818580627441),\n",
       "  ('Alan Lathwell', 0.7691977024078369),\n",
       "  ('Laurie Gwen Shapiro', 0.7670925855636597)],\n",
       " [('May 05, 2016', 0.9181225299835205),\n",
       "  ('May 5, 2016', 0.9158812761306763),\n",
       "  ('May 7, 2013', 0.9122775793075562),\n",
       "  ('May 25, 2016', 0.9120886921882629),\n",
       "  ('May 6, 2016', 0.9098081588745117),\n",
       "  ('May 7, 2016', 0.9093211889266968),\n",
       "  ('May 14, 2013', 0.9086767435073853),\n",
       "  ('May 5, 2015', 0.9072896242141724),\n",
       "  ('May 05, 2015', 0.9059444069862366),\n",
       "  ('May 5, 2014', 0.9057948589324951)],\n",
       " [('The Coming of Conan the Cimmerian: The Original Adventures of the Greatest Sword and Sorcery Hero of All Time!',\n",
       "   0.9045103192329407),\n",
       "  ('The Collected Fiction, Vol. 1: The Boats of the \"Glen Carrig\" and Other Nautical Adventures',\n",
       "   0.8999711275100708),\n",
       "  ('Form and Function: The Fantasy Epic (Aulds of the SPYRE Book 1)',\n",
       "   0.8967214822769165),\n",
       "  ('The Necromancer Candle: And Two Additional Tales of Contemporary Fantasy',\n",
       "   0.8945128321647644),\n",
       "  ('The Seduction of the Mediterranean: Writing, Art and Homosexual Fantasy',\n",
       "   0.8943796753883362),\n",
       "  ('Truth Behind the Fantasy of Porn: The Greatest Illusion on Earth',\n",
       "   0.8889137506484985),\n",
       "  ('Archaeologies of the Future: The Desire Called Utopia and Other Science Fictions',\n",
       "   0.8879579305648804),\n",
       "  (\"The Lady and the Panda: The True Adventures of the First American Explorer to Bring Back China's Most Exotic Animal\",\n",
       "   0.8874179720878601),\n",
       "  (\"Nebula Awards 33: The year's best SF and fantasy chosen by the Science-fiction and Fantasy Writers of America\",\n",
       "   0.8858234286308289),\n",
       "  ('The Weird: A Compendium of Dark and Strange Fictions',\n",
       "   0.8847765326499939)],\n",
       " [('Epic Books Press; 1 edition', 0.8946378231048584),\n",
       "  ('Wave One Publishing; Book 1 ed. edition', 0.8842207789421082),\n",
       "  ('Scholastic Press; Box edition', 0.8791025876998901),\n",
       "  ('Scholastic Press; 9.1.2013 edition', 0.8715577125549316),\n",
       "  ('Silver Hawk Press, LLC; First edition', 0.8715106248855591),\n",
       "  ('North Light Books; 3 edition', 0.86296546459198),\n",
       "  ('Fall River Press, NY, for Quid Publishing, UK.', 0.860957145690918),\n",
       "  ('Mercury Press; Monthly edition', 0.8551900386810303),\n",
       "  ('Selene River Press, Inc; 2nd edition', 0.8534359335899353),\n",
       "  ('Thomas Nelson Publishing; 1 edition', 0.8528700470924377)],\n",
       " [('May 4, 2004', 0.8968212604522705),\n",
       "  ('May 6, 2003', 0.8883941173553467),\n",
       "  ('May 06, 2003', 0.8863556981086731),\n",
       "  ('May 3, 2000', 0.8839065432548523),\n",
       "  ('May 25, 2004', 0.8835892081260681),\n",
       "  ('May 19, 2002', 0.8832105398178101),\n",
       "  ('May 07, 2001', 0.8814902305603027),\n",
       "  ('May 05, 2005', 0.8799021244049072),\n",
       "  ('July 31, 2003', 0.877388596534729),\n",
       "  ('May 3, 2005', 0.8757020235061646)]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getClosestWordEmbedding(fastTextModelPath, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
